<!--
 Copyright (C) 2018-2019 Intel Corporation
 SPDX-License-Identifier: Apache-2.0
-->

<!--
// ===============================================================================
// Generated file for Inference Engine extension for GPU plugin
//
// Contains configuration for the GPU layers
// Should be placed near executable that will load and use it
//
// This configuration file should be manually loaded in the code **before loading
// the model** that contains custom layers:
//      1. Use IInferencePlugin::SetConfig() method with following pair:
//          * PluginConfigParams::KEY_CONFIG_FILE
//          * configuration file name
//
// Example:
//      // Load clDNN (GPU) plugin
//      InferenceEngine::InferenceEnginePluginPtr plugin_ptr(selectPlugin({..., "GPU"));
//      InferencePlugin plugin(plugin_ptr);
//      // Load clDNN Extensions
//      plugin.SetConfig({{PluginConfigParams::KEY_CONFIG_FILE, "<path to the xml file>"}});
//
// You should fill :
//     * Buffers section
//       Add information about inputs/outputs
//     * Define global worksize for your layer
//
// Refer to the section "Adding Your Own Kernels to the Inference Engine" in
// OpenVINO* documentation (either online or offline in
// <INSTALL_DIR>/deployment_tools/documentation/docs/index.html an then navigate
// to the corresponding section).
// ===============================================================================
-->

[[[cog
from ext_gen.interactive_module import InteractiveModule

name = InteractiveModule.get_param('opName')
cog.outl("<CustomLayer name=\"%s\" type=\"SimpleGPU\" version=\"1\">" % (name))
cog.outl("    <Kernel entry=\"%s_kernel\">" % (name.lower()))
cog.outl("        <Source filename=\"%s_kernel.cl\"/>" % (name.lower()))
cog.outl("        <!-- Parameters description /-->")
params = InteractiveModule.get_param('params_gpu')
for p in params :
    cog.outl("        <Define name=\"%s\" type=\"%s\" param=\"%s\" default=\"%s\"/>" % (p[0], p[1], p[0], p[2]))
]]]
[[[end]]]
    </Kernel>
    <!-- Buffer descriptions /-->
    <Buffers>
        <!--Tensor arg-index="0" type="input" port-index="0"  format="BFYX"/-->
        <!--Tensor arg-index="1" type="output" port-index="0" format="BFYX"/-->
    </Buffers>
  
    <CompilerOptions options="-cl-mad-enable"/>
    <!-- define the global worksize. The formulas can use the values of the B,F,Y,X dimensions and contain the operators: +,-,/,*,% 
         (all evaluated in integer arithmetics). Default value: global="B*F*Y*X,1,1"/-->
    <!--WorkSizes global="X,Y,B*F"/--> 
</CustomLayer>
